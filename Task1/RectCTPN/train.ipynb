{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31e82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from loss import *\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0,  os.path.dirname(os.path.dirname(os.getcwd()) ))\n",
    "\n",
    "from DataLoader import SROIEDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02312ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctpn_collate_fn(batch):\n",
    "    \"\"\"Collate функция для CTPN\"\"\"\n",
    "    images = []\n",
    "    boxes_list = []\n",
    "    original_quads_list = []\n",
    "    texts_list = []\n",
    "    image_ids = []\n",
    "    original_sizes = []\n",
    "    scale_factors = []\n",
    "    \n",
    "    for item in batch:\n",
    "        images.append(item['image'])\n",
    "        boxes_list.append(torch.tensor(item['boxes'], dtype=torch.float32))\n",
    "        original_quads_list.append(item['original_quads'])\n",
    "        texts_list.append(item['texts'])\n",
    "        image_ids.append(item['image_id'])\n",
    "        original_sizes.append(item['original_size'])\n",
    "        scale_factors.append(item['scale_factors'])\n",
    "    \n",
    "    # Стекируем изображения одинакового размера\n",
    "    images = torch.stack(images, dim=0)\n",
    "    \n",
    "    return {\n",
    "        'images': images,  # [B, 3, H, W]\n",
    "        'boxes': boxes_list,  # список тензоров [N_i, 4]\n",
    "        'original_quads': original_quads_list,\n",
    "        'texts': texts_list,\n",
    "        'image_ids': image_ids,\n",
    "        'original_sizes': original_sizes,\n",
    "        'scale_factors': scale_factors\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a280e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SROIEDataset('../../data/test/img','../../data/test/box', target_size=(640, 640))\n",
    "\n",
    "sub_train_dataset = [train_dataset[i] for i in range(10)]\n",
    "\n",
    "train_loader = DataLoader(sub_train_dataset, batch_size=5, collate_fn=ctpn_collate_fn,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a7e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714044d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'boxes', 'original_quads', 'texts', 'image_ids', 'original_sizes', 'scale_factors'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d299cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 640, 640])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfa5f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone output: 64 channels, stride: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: stride=2, feature map: 320x320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g_/4txfl8x17kv9txsffh9p9xf00000gn/T/ipykernel_79420/1679872034.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dh = torch.log(torch.tensor(gt_height / anchor_h, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0 - Shape Debug:\n",
      "  Images: torch.Size([5, 3, 640, 640])\n",
      "  cls_pred: torch.Size([5, 20, 320, 320]), cls_target: torch.Size([5, 320, 320, 20])\n",
      "  reg_pred: torch.Size([5, 20, 320, 320]), reg_target: torch.Size([5, 320, 320, 20])\n",
      "  side_pred: torch.Size([5, 20, 320, 320]), side_target: torch.Size([5, 320, 320, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 1/2 [00:17<00:17, 17.42s/it, loss=0.707, cls=0.705, reg=1.27e-6, side=0.00165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 0 Stats:\n",
      "    Positive anchors: 100.6 per image\n",
      "    Anchor ratio: 0.01%\n",
      "    Losses - Total: 0.7068, Cls: 0.7052, Reg: 0.0000, Side: 0.0017\n",
      "Targets: stride=2, feature map: 320x320\n",
      "\n",
      "Batch 1 - Shape Debug:\n",
      "  Images: torch.Size([5, 3, 640, 640])\n",
      "  cls_pred: torch.Size([5, 20, 320, 320]), cls_target: torch.Size([5, 320, 320, 20])\n",
      "  reg_pred: torch.Size([5, 20, 320, 320]), reg_target: torch.Size([5, 320, 320, 20])\n",
      "  side_pred: torch.Size([5, 20, 320, 320]), side_target: torch.Size([5, 320, 320, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:42<00:00, 21.07s/it, loss=0.679, cls=0.677, reg=1.51e-6, side=0.002]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6927269101142883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: stride=2, feature map: 320x320\n",
      "\n",
      "Batch 0 - Shape Debug:\n",
      "  Images: torch.Size([5, 3, 640, 640])\n",
      "  cls_pred: torch.Size([5, 20, 320, 320]), cls_target: torch.Size([5, 320, 320, 20])\n",
      "  reg_pred: torch.Size([5, 20, 320, 320]), reg_target: torch.Size([5, 320, 320, 20])\n",
      "  side_pred: torch.Size([5, 20, 320, 320]), side_target: torch.Size([5, 320, 320, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  50%|█████     | 1/2 [00:15<00:15, 15.69s/it, loss=0.637, cls=0.635, reg=1.75e-6, side=0.00165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 0 Stats:\n",
      "    Positive anchors: 100.6 per image\n",
      "    Anchor ratio: 0.01%\n",
      "    Losses - Total: 0.6370, Cls: 0.6354, Reg: 0.0000, Side: 0.0017\n",
      "Targets: stride=2, feature map: 320x320\n",
      "\n",
      "Batch 1 - Shape Debug:\n",
      "  Images: torch.Size([5, 3, 640, 640])\n",
      "  cls_pred: torch.Size([5, 20, 320, 320]), cls_target: torch.Size([5, 320, 320, 20])\n",
      "  reg_pred: torch.Size([5, 20, 320, 320]), reg_target: torch.Size([5, 320, 320, 20])\n",
      "  side_pred: torch.Size([5, 20, 320, 320]), side_target: torch.Size([5, 320, 320, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2/2 [00:30<00:00, 15.47s/it, loss=0.576, cls=0.574, reg=2.16e-6, side=0.002]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6064310669898987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: stride=2, feature map: 320x320\n",
      "\n",
      "Batch 0 - Shape Debug:\n",
      "  Images: torch.Size([5, 3, 640, 640])\n",
      "  cls_pred: torch.Size([5, 20, 320, 320]), cls_target: torch.Size([5, 320, 320, 20])\n",
      "  reg_pred: torch.Size([5, 20, 320, 320]), reg_target: torch.Size([5, 320, 320, 20])\n",
      "  side_pred: torch.Size([5, 20, 320, 320]), side_target: torch.Size([5, 320, 320, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  50%|█████     | 1/2 [00:16<00:16, 16.16s/it, loss=0.506, cls=0.504, reg=2.18e-6, side=0.00165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 0 Stats:\n",
      "    Positive anchors: 100.6 per image\n",
      "    Anchor ratio: 0.01%\n",
      "    Losses - Total: 0.5061, Cls: 0.5044, Reg: 0.0000, Side: 0.0016\n",
      "Targets: stride=2, feature map: 320x320\n",
      "\n",
      "Batch 1 - Shape Debug:\n",
      "  Images: torch.Size([5, 3, 640, 640])\n",
      "  cls_pred: torch.Size([5, 20, 320, 320]), cls_target: torch.Size([5, 320, 320, 20])\n",
      "  reg_pred: torch.Size([5, 20, 320, 320]), reg_target: torch.Size([5, 320, 320, 20])\n",
      "  side_pred: torch.Size([5, 20, 320, 320]), side_target: torch.Size([5, 320, 320, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:32<00:00, 16.10s/it, loss=0.436, cls=0.434, reg=3.01e-6, side=0.00199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4710754007101059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Пример использования с полными таргетами\n",
    "def prepare_full_ctpn_targets(batch, num_anchors=10):\n",
    "    \"\"\"Таргеты для stride=2 (320x320 feature map)\"\"\"\n",
    "    anchor_scales = [3, 4, 6, 8, 12, 17, 24, 35, 50, 71]  # Оригинальные / 4\n",
    "    \n",
    "    images = batch['images']\n",
    "    boxes_list = batch['boxes']\n",
    "    \n",
    "    B, C, H, W = images.shape  # [B, 3, 640, 640]\n",
    "    feature_stride = 2\n",
    "    feat_h, feat_w = H // feature_stride, W // feature_stride  # 320x320\n",
    "    \n",
    "    print(f\"Targets: stride={feature_stride}, feature map: {feat_h}x{feat_w}\")\n",
    "    \n",
    "    cls_targets = torch.zeros(B, feat_h, feat_w, num_anchors, 2, dtype=torch.float32)\n",
    "    reg_targets = torch.zeros(B, feat_h, feat_w, num_anchors, 2, dtype=torch.float32)\n",
    "    side_targets = torch.zeros(B, feat_h, feat_w, num_anchors, 2, dtype=torch.float32)\n",
    "    \n",
    "    for b_idx in range(B):\n",
    "        boxes = boxes_list[b_idx]\n",
    "        \n",
    "        for box in boxes:\n",
    "            if len(box) == 0:\n",
    "                continue\n",
    "            \n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            gt_height = y_max - y_min\n",
    "            gt_center_y = (y_min + y_max) / 2\n",
    "            gt_center_x = (x_min + x_max) / 2\n",
    "            \n",
    "            # Координаты на feature map\n",
    "            feat_x = min(int(gt_center_x // feature_stride), feat_w - 1)\n",
    "            feat_y = min(int(gt_center_y // feature_stride), feat_h - 1)\n",
    "            \n",
    "            for a_idx, anchor_h in enumerate(anchor_scales):\n",
    "                anchor_center_y = (feat_y + 0.5) * feature_stride\n",
    "                \n",
    "                # Vertical IoU\n",
    "                anchor_y_min = anchor_center_y - anchor_h / 2\n",
    "                anchor_y_max = anchor_center_y + anchor_h / 2\n",
    "                \n",
    "                y_min_inter = max(anchor_y_min, y_min)\n",
    "                y_max_inter = min(anchor_y_max, y_max)\n",
    "                inter_h = max(0, y_max_inter - y_min_inter)\n",
    "                \n",
    "                union_h = (anchor_y_max - anchor_y_min) + (y_max - y_min) - inter_h\n",
    "                vertical_iou = inter_h / union_h if union_h > 0 else 0\n",
    "                \n",
    "                if vertical_iou > 0.7:\n",
    "                    cls_targets[b_idx, feat_y, feat_x, a_idx] = torch.tensor([0., 1.])\n",
    "                    \n",
    "                    dy = (gt_center_y - anchor_center_y) / anchor_h\n",
    "                    dh = torch.log(torch.tensor(gt_height / anchor_h, dtype=torch.float32))\n",
    "                    \n",
    "                    reg_targets[b_idx, feat_y, feat_x, a_idx, 0] = dy\n",
    "                    reg_targets[b_idx, feat_y, feat_x, a_idx, 1] = dh\n",
    "                    \n",
    "                    dx_left = (x_min - feat_x * feature_stride) / feature_stride\n",
    "                    dx_right = (x_max - feat_x * feature_stride) / feature_stride\n",
    "                    \n",
    "                    side_targets[b_idx, feat_y, feat_x, a_idx, 0] = dx_left\n",
    "                    side_targets[b_idx, feat_y, feat_x, a_idx, 1] = dx_right\n",
    "                    \n",
    "                elif vertical_iou < 0.3:\n",
    "                    cls_targets[b_idx, feat_y, feat_x, a_idx] = torch.tensor([1., 0.])\n",
    "    \n",
    "    cls_targets = cls_targets.view(B, feat_h, feat_w, -1)  # [B, 320, 320, 20]\n",
    "    reg_targets = reg_targets.view(B, feat_h, feat_w, -1)\n",
    "    side_targets = side_targets.view(B, feat_h, feat_w, -1)\n",
    "    \n",
    "    return {\n",
    "        'cls_targets': cls_targets,\n",
    "        'reg_targets': reg_targets,\n",
    "        'side_targets': side_targets\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch, num_anchors=10):\n",
    "    \"\"\"Одна эпоха обучения для полной CTPN модели\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    cls_loss_sum = 0\n",
    "    reg_loss_sum = 0\n",
    "    side_loss_sum = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # 1. Перенос изображений на устройство\n",
    "            images = batch['images'].to(device)\n",
    "            \n",
    "            # 2. Подготовка полных таргетов для CTPN\n",
    "            gt_data = prepare_full_ctpn_targets(\n",
    "                batch, \n",
    "                num_anchors=num_anchors, # VGG16 stride\n",
    "            )\n",
    "            \n",
    "            # 3. Forward pass - получаем 3 выхода\n",
    "            optimizer.zero_grad()\n",
    "            cls_pred, reg_pred, side_pred = model(images)\n",
    "            \n",
    "            # 4. Проверка размеров (для отладки)\n",
    "            B, C_cls, H_cls, W_cls = cls_pred.shape\n",
    "            B, C_reg, H_reg, W_reg = reg_pred.shape\n",
    "            B, C_side, H_side, W_side = side_pred.shape\n",
    "            \n",
    "            cls_target = gt_data['cls_targets']\n",
    "            reg_target = gt_data['reg_targets']\n",
    "            side_target = gt_data['side_targets']\n",
    "            \n",
    "            print(f\"\\nBatch {batch_idx} - Shape Debug:\")\n",
    "            print(f\"  Images: {images.shape}\")\n",
    "            print(f\"  cls_pred: {cls_pred.shape}, cls_target: {cls_target.shape}\")\n",
    "            print(f\"  reg_pred: {reg_pred.shape}, reg_target: {reg_target.shape}\")\n",
    "            print(f\"  side_pred: {side_pred.shape}, side_target: {side_target.shape}\")\n",
    "            \n",
    "            # Проверяем соответствие размеров\n",
    "            expected_h = images.shape[2] // 2  # feature map height\n",
    "            expected_w = images.shape[3] // 2  # feature map width\n",
    "            \n",
    "            if H_cls != expected_h or W_cls != expected_w:\n",
    "                print(f\"  WARNING: cls_pred имеет размер {H_cls}x{W_cls}, ожидалось {expected_h}x{expected_w}\")\n",
    "                # Делаем ресайз если нужно\n",
    "                cls_pred = F.interpolate(cls_pred, size=(expected_h, expected_w), mode='bilinear')\n",
    "                reg_pred = F.interpolate(reg_pred, size=(expected_h, expected_w), mode='bilinear')\n",
    "                side_pred = F.interpolate(side_pred, size=(expected_h, expected_w), mode='bilinear')\n",
    "            \n",
    "            # 5. Вычисление loss\n",
    "            losses = criterion(cls_pred, reg_pred, side_pred, gt_data)\n",
    "            \n",
    "            # 6. Backward pass\n",
    "            losses['total'].backward()\n",
    "            \n",
    "            # 7. Gradient clipping (важно для RNN)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            \n",
    "            # 8. Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 9. Статистика\n",
    "            total_loss += losses['total'].item()\n",
    "            cls_loss_sum += losses['cls'].item()\n",
    "            reg_loss_sum += losses['reg'].item()\n",
    "            side_loss_sum += losses['side'].item()\n",
    "            \n",
    "            # 10. Обновление progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': losses['total'].item(),\n",
    "                'cls': losses['cls'].item(),\n",
    "                'reg': losses['reg'].item(),\n",
    "                'side': losses['side'].item()\n",
    "            })\n",
    "            \n",
    "            # 11. Логирование каждые 10 батчей\n",
    "            if batch_idx % 10 == 0:\n",
    "                # Проверяем количество позитивных anchor'ов\n",
    "                pos_anchors = (gt_data['cls_targets'][..., 1::2].sum() / B).item()\n",
    "                total_anchors = cls_target.shape[1] * cls_target.shape[2] * num_anchors\n",
    "                \n",
    "                print(f\"\\n  Batch {batch_idx} Stats:\")\n",
    "                print(f\"    Positive anchors: {pos_anchors:.1f} per image\")\n",
    "                print(f\"    Anchor ratio: {pos_anchors/total_anchors*100:.2f}%\")\n",
    "                print(f\"    Losses - Total: {losses['total'].item():.4f}, \"\n",
    "                      f\"Cls: {losses['cls'].item():.4f}, \"\n",
    "                      f\"Reg: {losses['reg'].item():.4f}, \"\n",
    "                      f\"Side: {losses['side'].item():.4f}\")\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in batch {batch_idx}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # 12. Вычисляем средние losses за эпоху\n",
    "    num_batches = max(len(dataloader), 1)\n",
    "    avg_total_loss = total_loss / num_batches\n",
    "    avg_cls_loss = cls_loss_sum / num_batches\n",
    "    avg_reg_loss = reg_loss_sum / num_batches\n",
    "    avg_side_loss = side_loss_sum / num_batches\n",
    "    \n",
    "    return avg_total_loss, avg_cls_loss, avg_reg_loss, avg_side_loss\n",
    "\n",
    "# Конфигурация обучения\n",
    "config = {\n",
    "    'num_epochs': 2,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'batch_size': 16,\n",
    "    'step_size': 10,\n",
    "    'gamma': 0.1,\n",
    "\n",
    "}\n",
    "\n",
    "# Создание модели\n",
    "model = CustomCTPN(5)\n",
    "\n",
    "# Разделение датасета на train/val\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = CTPNLoss()\n",
    "    \n",
    "optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config.get('lr', 0.001),\n",
    "        weight_decay=config.get('weight_decay', 0.0001)\n",
    "    )\n",
    "# Запуск обучения\n",
    "history = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "     avg_total_loss, avg_cls_loss, avg_reg_loss, avg_side_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch=i+1)\n",
    "     print(avg_total_loss)\n",
    "     history.append((avg_total_loss, avg_reg_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a235add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6927269101142883, 1.392185311033245e-06),\n",
       " (0.6064310669898987, 1.9550967067516467e-06),\n",
       " (0.4710754007101059, 2.597034608697868e-06)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b89f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
